{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae0b12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\chawk\\AppData\\Local\\Temp\\ipykernel_7228\\1920312512.py:12: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  Dataset =  \"D:\\model\\stegano\\dataset\\for-2seconds\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "Dataset =  \"D:\\model\\stegano\\dataset\\for-2seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40cbddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def preprocess_data(dataset_path):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for variant in os.listdir(dataset_path):\n",
    "        variant_path = os.path.join(dataset_path, variant)\n",
    "        for classesz in os.listdir(variant_path):\n",
    "            classes_path = os.path.join(variant_path, classesz)\n",
    "            for audio in os.listdir(classes_path):\n",
    "                audio_path = os.path.join(classes_path, audio)\n",
    "                \n",
    "                try:\n",
    "                    raw_audio = AudioSegment.from_file(audio_path)\n",
    "                    \n",
    "                    samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n",
    "                    trimmed, _ = librosa.effects.trim(samples, top_db=25)\n",
    "                    padding = max(0, 50000 - len(trimmed))\n",
    "                    padded = np.pad(trimmed, (0, padding), 'constant')\n",
    "\n",
    "                    X.append(padded)\n",
    "                    y.append(classesz)\n",
    "                                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Processing error {audio_path}: {e}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "Dataset = r\"D:\\model\\stegano\\dataset\\for-2seconds\"\n",
    "X, y = preprocess_data(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2a1c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y):\n",
    "    encoded_labels = []\n",
    "    for label in y:\n",
    "        if label == 'real':\n",
    "            encoded_labels.append(1)\n",
    "        elif label == 'fake':\n",
    "            encoded_labels.append(0)\n",
    "    return encoded_labels\n",
    "\n",
    "y_encoded = encode_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa0293b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59907f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X):\n",
    "    zcr_list = []\n",
    "    rms_list = []\n",
    "    mfccs_list = []\n",
    "\n",
    "    FRAME_LENGTH = 2048\n",
    "    HOP_LENGTH = 512\n",
    "\n",
    "    for audio in X:\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        rms = librosa.feature.rms(y=audio, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=44100, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "\n",
    "        zcr_list.append(zcr)\n",
    "        rms_list.append(rms)\n",
    "        mfccs_list.append(mfccs)\n",
    "\n",
    "    return zcr_list, rms_list, mfccs_list\n",
    "\n",
    "zcr_features, rms_features, mfccs_features = extract_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bd6d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(zcr_list, rms_list, mfccs_list):\n",
    "    zcr_features = np.swapaxes(zcr_list, 1, 2)\n",
    "    rms_features = np.swapaxes(rms_list, 1, 2)\n",
    "    mfccs_features = np.swapaxes(mfccs_list, 1, 2)\n",
    "\n",
    "    X_features = np.concatenate((zcr_features, rms_features, mfccs_features), axis=2)\n",
    "\n",
    "    return X_features\n",
    "\n",
    "X_features = combine_features(zcr_features, rms_features, mfccs_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efc0e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chawk\\anaconda3\\envs\\ai\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.7103 - loss: 0.5724\n",
      "Epoch 1: val_loss improved from inf to 0.43492, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 47ms/step - categorical_accuracy: 0.7103 - loss: 0.5724 - val_categorical_accuracy: 0.8063 - val_loss: 0.4349 - learning_rate: 0.0010\n",
      "Epoch 2/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.7836 - loss: 0.4727\n",
      "Epoch 2: val_loss improved from 0.43492 to 0.40549, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 47ms/step - categorical_accuracy: 0.7836 - loss: 0.4727 - val_categorical_accuracy: 0.8119 - val_loss: 0.4055 - learning_rate: 0.0010\n",
      "Epoch 3/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.8170 - loss: 0.4211\n",
      "Epoch 3: val_loss improved from 0.40549 to 0.36461, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 47ms/step - categorical_accuracy: 0.8170 - loss: 0.4211 - val_categorical_accuracy: 0.8308 - val_loss: 0.3646 - learning_rate: 0.0010\n",
      "Epoch 4/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.8362 - loss: 0.3863\n",
      "Epoch 4: val_loss improved from 0.36461 to 0.35206, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 48ms/step - categorical_accuracy: 0.8362 - loss: 0.3863 - val_categorical_accuracy: 0.8436 - val_loss: 0.3521 - learning_rate: 0.0010\n",
      "Epoch 5/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.8170 - loss: 0.4060\n",
      "Epoch 5: val_loss did not improve from 0.35206\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 50ms/step - categorical_accuracy: 0.8170 - loss: 0.4060 - val_categorical_accuracy: 0.7994 - val_loss: 0.4330 - learning_rate: 0.0010\n",
      "Epoch 6/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.8363 - loss: 0.3784\n",
      "Epoch 6: val_loss improved from 0.35206 to 0.33088, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 52ms/step - categorical_accuracy: 0.8363 - loss: 0.3784 - val_categorical_accuracy: 0.8641 - val_loss: 0.3309 - learning_rate: 0.0010\n",
      "Epoch 7/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - categorical_accuracy: 0.8332 - loss: 0.3736\n",
      "Epoch 7: val_loss did not improve from 0.33088\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 55ms/step - categorical_accuracy: 0.8332 - loss: 0.3736 - val_categorical_accuracy: 0.8579 - val_loss: 0.3358 - learning_rate: 0.0010\n",
      "Epoch 8/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.8467 - loss: 0.3496\n",
      "Epoch 8: val_loss improved from 0.33088 to 0.24568, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 53ms/step - categorical_accuracy: 0.8467 - loss: 0.3496 - val_categorical_accuracy: 0.8986 - val_loss: 0.2457 - learning_rate: 0.0010\n",
      "Epoch 9/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - categorical_accuracy: 0.8700 - loss: 0.3063\n",
      "Epoch 9: val_loss improved from 0.24568 to 0.23828, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 52ms/step - categorical_accuracy: 0.8700 - loss: 0.3063 - val_categorical_accuracy: 0.9036 - val_loss: 0.2383 - learning_rate: 0.0010\n",
      "Epoch 10/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.8755 - loss: 0.2898\n",
      "Epoch 10: val_loss improved from 0.23828 to 0.23769, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 53ms/step - categorical_accuracy: 0.8755 - loss: 0.2898 - val_categorical_accuracy: 0.8989 - val_loss: 0.2377 - learning_rate: 0.0010\n",
      "Epoch 11/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.8836 - loss: 0.2838\n",
      "Epoch 11: val_loss improved from 0.23769 to 0.21935, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 46ms/step - categorical_accuracy: 0.8836 - loss: 0.2837 - val_categorical_accuracy: 0.9101 - val_loss: 0.2194 - learning_rate: 0.0010\n",
      "Epoch 12/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.8902 - loss: 0.2715\n",
      "Epoch 12: val_loss did not improve from 0.21935\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 43ms/step - categorical_accuracy: 0.8902 - loss: 0.2715 - val_categorical_accuracy: 0.8909 - val_loss: 0.2574 - learning_rate: 0.0010\n",
      "Epoch 13/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.8875 - loss: 0.2624\n",
      "Epoch 13: val_loss did not improve from 0.21935\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 53ms/step - categorical_accuracy: 0.8875 - loss: 0.2624 - val_categorical_accuracy: 0.9114 - val_loss: 0.2233 - learning_rate: 0.0010\n",
      "Epoch 14/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.9042 - loss: 0.2474\n",
      "Epoch 14: val_loss improved from 0.21935 to 0.18778, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 48ms/step - categorical_accuracy: 0.9042 - loss: 0.2474 - val_categorical_accuracy: 0.9238 - val_loss: 0.1878 - learning_rate: 0.0010\n",
      "Epoch 15/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - categorical_accuracy: 0.8957 - loss: 0.2655\n",
      "Epoch 15: val_loss did not improve from 0.18778\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 52ms/step - categorical_accuracy: 0.8957 - loss: 0.2655 - val_categorical_accuracy: 0.9182 - val_loss: 0.2057 - learning_rate: 0.0010\n",
      "Epoch 16/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.9009 - loss: 0.2449\n",
      "Epoch 16: val_loss did not improve from 0.18778\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 50ms/step - categorical_accuracy: 0.9009 - loss: 0.2449 - val_categorical_accuracy: 0.9108 - val_loss: 0.2004 - learning_rate: 0.0010\n",
      "Epoch 17/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.9055 - loss: 0.2372\n",
      "Epoch 17: val_loss improved from 0.18778 to 0.17801, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 46ms/step - categorical_accuracy: 0.9055 - loss: 0.2372 - val_categorical_accuracy: 0.9316 - val_loss: 0.1780 - learning_rate: 0.0010\n",
      "Epoch 18/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.9134 - loss: 0.2188\n",
      "Epoch 18: val_loss did not improve from 0.17801\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 48ms/step - categorical_accuracy: 0.9134 - loss: 0.2188 - val_categorical_accuracy: 0.8980 - val_loss: 0.2873 - learning_rate: 0.0010\n",
      "Epoch 19/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.9098 - loss: 0.2336\n",
      "Epoch 19: val_loss did not improve from 0.17801\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 49ms/step - categorical_accuracy: 0.9098 - loss: 0.2335 - val_categorical_accuracy: 0.9132 - val_loss: 0.2646 - learning_rate: 0.0010\n",
      "Epoch 20/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.9111 - loss: 0.2264\n",
      "Epoch 20: val_loss did not improve from 0.17801\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 47ms/step - categorical_accuracy: 0.9111 - loss: 0.2264 - val_categorical_accuracy: 0.9235 - val_loss: 0.2039 - learning_rate: 0.0010\n",
      "Epoch 21/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.9120 - loss: 0.2297\n",
      "Epoch 21: val_loss improved from 0.17801 to 0.17315, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 50ms/step - categorical_accuracy: 0.9120 - loss: 0.2297 - val_categorical_accuracy: 0.9288 - val_loss: 0.1731 - learning_rate: 0.0010\n",
      "Epoch 22/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - categorical_accuracy: 0.9171 - loss: 0.2145\n",
      "Epoch 22: val_loss improved from 0.17315 to 0.16818, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 49ms/step - categorical_accuracy: 0.9171 - loss: 0.2145 - val_categorical_accuracy: 0.9372 - val_loss: 0.1682 - learning_rate: 0.0010\n",
      "Epoch 23/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.9177 - loss: 0.2174\n",
      "Epoch 23: val_loss did not improve from 0.16818\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 47ms/step - categorical_accuracy: 0.9177 - loss: 0.2174 - val_categorical_accuracy: 0.9291 - val_loss: 0.1801 - learning_rate: 0.0010\n",
      "Epoch 24/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.9164 - loss: 0.2178\n",
      "Epoch 24: val_loss did not improve from 0.16818\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 51ms/step - categorical_accuracy: 0.9164 - loss: 0.2178 - val_categorical_accuracy: 0.9350 - val_loss: 0.1801 - learning_rate: 0.0010\n",
      "Epoch 25/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.9189 - loss: 0.2176\n",
      "Epoch 25: val_loss did not improve from 0.16818\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 46ms/step - categorical_accuracy: 0.9189 - loss: 0.2176 - val_categorical_accuracy: 0.9341 - val_loss: 0.1797 - learning_rate: 0.0010\n",
      "Epoch 26/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.9164 - loss: 0.2278\n",
      "Epoch 26: val_loss improved from 0.16818 to 0.14479, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 50ms/step - categorical_accuracy: 0.9164 - loss: 0.2278 - val_categorical_accuracy: 0.9462 - val_loss: 0.1448 - learning_rate: 0.0010\n",
      "Epoch 27/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.9234 - loss: 0.2058\n",
      "Epoch 27: val_loss did not improve from 0.14479\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 51ms/step - categorical_accuracy: 0.9234 - loss: 0.2058 - val_categorical_accuracy: 0.9344 - val_loss: 0.1742 - learning_rate: 0.0010\n",
      "Epoch 28/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - categorical_accuracy: 0.9255 - loss: 0.1988\n",
      "Epoch 28: val_loss did not improve from 0.14479\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 51ms/step - categorical_accuracy: 0.9255 - loss: 0.1988 - val_categorical_accuracy: 0.9288 - val_loss: 0.1826 - learning_rate: 0.0010\n",
      "Epoch 29/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - categorical_accuracy: 0.9261 - loss: 0.2003\n",
      "Epoch 29: val_loss did not improve from 0.14479\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 53ms/step - categorical_accuracy: 0.9261 - loss: 0.2003 - val_categorical_accuracy: 0.9328 - val_loss: 0.1629 - learning_rate: 0.0010\n",
      "Epoch 30/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - categorical_accuracy: 0.9177 - loss: 0.2158\n",
      "Epoch 30: val_loss did not improve from 0.14479\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 51ms/step - categorical_accuracy: 0.9177 - loss: 0.2158 - val_categorical_accuracy: 0.9397 - val_loss: 0.1626 - learning_rate: 0.0010\n",
      "Epoch 31/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.9207 - loss: 0.2068\n",
      "Epoch 31: val_loss improved from 0.14479 to 0.14477, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 44ms/step - categorical_accuracy: 0.9207 - loss: 0.2068 - val_categorical_accuracy: 0.9515 - val_loss: 0.1448 - learning_rate: 0.0010\n",
      "Epoch 32/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - categorical_accuracy: 0.9296 - loss: 0.1956\n",
      "Epoch 32: val_loss did not improve from 0.14477\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 41ms/step - categorical_accuracy: 0.9296 - loss: 0.1956 - val_categorical_accuracy: 0.9391 - val_loss: 0.1699 - learning_rate: 0.0010\n",
      "Epoch 33/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - categorical_accuracy: 0.9204 - loss: 0.2091\n",
      "Epoch 33: val_loss did not improve from 0.14477\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 41ms/step - categorical_accuracy: 0.9204 - loss: 0.2091 - val_categorical_accuracy: 0.9422 - val_loss: 0.1668 - learning_rate: 0.0010\n",
      "Epoch 34/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - categorical_accuracy: 0.9266 - loss: 0.2108\n",
      "Epoch 34: val_loss did not improve from 0.14477\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 42ms/step - categorical_accuracy: 0.9266 - loss: 0.2108 - val_categorical_accuracy: 0.9450 - val_loss: 0.1502 - learning_rate: 0.0010\n",
      "Epoch 35/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.9301 - loss: 0.1975\n",
      "Epoch 35: val_loss improved from 0.14477 to 0.13984, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 47ms/step - categorical_accuracy: 0.9301 - loss: 0.1975 - val_categorical_accuracy: 0.9518 - val_loss: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 36/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.9243 - loss: 0.2000\n",
      "Epoch 36: val_loss did not improve from 0.13984\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 48ms/step - categorical_accuracy: 0.9243 - loss: 0.2000 - val_categorical_accuracy: 0.9347 - val_loss: 0.1753 - learning_rate: 0.0010\n",
      "Epoch 37/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - categorical_accuracy: 0.9248 - loss: 0.2162\n",
      "Epoch 37: val_loss did not improve from 0.13984\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 44ms/step - categorical_accuracy: 0.9248 - loss: 0.2162 - val_categorical_accuracy: 0.9419 - val_loss: 0.1737 - learning_rate: 0.0010\n",
      "Epoch 38/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.9229 - loss: 0.2088\n",
      "Epoch 38: val_loss improved from 0.13984 to 0.13977, saving model to CustomModel.keras\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 45ms/step - categorical_accuracy: 0.9229 - loss: 0.2088 - val_categorical_accuracy: 0.9468 - val_loss: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 39/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - categorical_accuracy: 0.9307 - loss: 0.1917\n",
      "Epoch 39: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 50ms/step - categorical_accuracy: 0.9307 - loss: 0.1918 - val_categorical_accuracy: 0.9279 - val_loss: 0.1970 - learning_rate: 0.0010\n",
      "Epoch 40/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - categorical_accuracy: 0.9178 - loss: 0.2221\n",
      "Epoch 40: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 46ms/step - categorical_accuracy: 0.9178 - loss: 0.2221 - val_categorical_accuracy: 0.9235 - val_loss: 0.2111 - learning_rate: 0.0010\n",
      "Epoch 41/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.9106 - loss: 0.2443\n",
      "Epoch 41: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 46ms/step - categorical_accuracy: 0.9106 - loss: 0.2443 - val_categorical_accuracy: 0.9366 - val_loss: 0.1753 - learning_rate: 0.0010\n",
      "Epoch 42/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - categorical_accuracy: 0.9243 - loss: 0.2074\n",
      "Epoch 42: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 50ms/step - categorical_accuracy: 0.9243 - loss: 0.2074 - val_categorical_accuracy: 0.9285 - val_loss: 0.1839 - learning_rate: 0.0010\n",
      "Epoch 43/70\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - categorical_accuracy: 0.9198 - loss: 0.2128\n",
      "Epoch 43: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 46ms/step - categorical_accuracy: 0.9198 - loss: 0.2128 - val_categorical_accuracy: 0.9387 - val_loss: 0.1717 - learning_rate: 0.0010\n",
      "Epoch 44/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - categorical_accuracy: 0.9221 - loss: 0.2077\n",
      "Epoch 44: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 47ms/step - categorical_accuracy: 0.9221 - loss: 0.2077 - val_categorical_accuracy: 0.9154 - val_loss: 0.2658 - learning_rate: 0.0010\n",
      "Epoch 45/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - categorical_accuracy: 0.9187 - loss: 0.2170\n",
      "Epoch 45: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 41ms/step - categorical_accuracy: 0.9187 - loss: 0.2170 - val_categorical_accuracy: 0.9223 - val_loss: 0.2289 - learning_rate: 0.0010\n",
      "Epoch 46/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - categorical_accuracy: 0.9188 - loss: 0.2144\n",
      "Epoch 46: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 40ms/step - categorical_accuracy: 0.9188 - loss: 0.2144 - val_categorical_accuracy: 0.9428 - val_loss: 0.1490 - learning_rate: 0.0010\n",
      "Epoch 47/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - categorical_accuracy: 0.9287 - loss: 0.2035\n",
      "Epoch 47: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 39ms/step - categorical_accuracy: 0.9287 - loss: 0.2035 - val_categorical_accuracy: 0.9316 - val_loss: 0.2059 - learning_rate: 0.0010\n",
      "Epoch 48/70\n",
      "\u001b[1m2084/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - categorical_accuracy: 0.9235 - loss: 0.2080\n",
      "Epoch 48: val_loss did not improve from 0.13977\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 41ms/step - categorical_accuracy: 0.9235 - loss: 0.2080 - val_categorical_accuracy: 0.9359 - val_loss: 0.1945 - learning_rate: 0.0010\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 38.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train_model(X_features, y):\n",
    "    X_train, X_to_split, y_train, y_to_split = train_test_split(X_features, y, test_size=0.3, random_state=1)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_to_split, y_to_split, test_size=0.4, random_state=1)\n",
    "\n",
    "    y_train_class = to_categorical(y_train, 2)\n",
    "    y_val_class = to_categorical(y_val, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(X_features.shape[1:3])))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='RMSProp', metrics=['categorical_accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('CustomModel.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.02, patience=100)\n",
    "\n",
    "    history = model.fit(X_train, y_train_class, epochs=70, batch_size=6, validation_data=(X_val, y_val_class), callbacks=[rlrop, early_stopping, checkpoint])\n",
    "\n",
    "    return model, history, X_test, y_test\n",
    "\n",
    "model, history, X_test, y_test = train_model(X_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58944312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1674717366695404\n",
      "Test Accuracy: 0.9310023188591003\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    y_test_class = to_categorical(y_test, 2)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test_class, verbose=0)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "test_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f520be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('deepfakedetectioncyber.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0eaa4229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio: file66.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav\n",
      "Prediction: fake\n",
      "Confidence: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fake', np.float32(0.9978618))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def test_existing_model(model, audio_path, is_directory=False):\n",
    "    \"\"\"\n",
    "    Test the existing trained model with new audio file(s).\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model object (already in memory)\n",
    "    - audio_path: Path to a single audio file or directory of audio files\n",
    "    - is_directory: Boolean indicating if audio_path is a directory\n",
    "    \n",
    "    Returns:\n",
    "    - Results of the predictions\n",
    "    \"\"\"\n",
    "    if is_directory:\n",
    "        return batch_test_existing_model(model, audio_path)\n",
    "    else:\n",
    "        return test_single_audio_existing_model(model, audio_path)\n",
    "\n",
    "def preprocess_single_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Preprocess a single audio file the same way as in training.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_path: Path to the audio file\n",
    "    \n",
    "    Returns:\n",
    "    - Feature vector ready for model input\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess audio\n",
    "        raw_audio = AudioSegment.from_file(audio_path)\n",
    "        samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n",
    "        trimmed, _ = librosa.effects.trim(samples, top_db=25)\n",
    "        padding = max(0, 50000 - len(trimmed))\n",
    "        padded = np.pad(trimmed, (0, padding), 'constant')\n",
    "        \n",
    "        # Extract features\n",
    "        FRAME_LENGTH = 2048\n",
    "        HOP_LENGTH = 512\n",
    "        \n",
    "        # Extract ZCR\n",
    "        zcr = librosa.feature.zero_crossing_rate(padded, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        zcr = np.expand_dims(zcr, axis=0)\n",
    "        \n",
    "        # Extract RMS\n",
    "        rms = librosa.feature.rms(y=padded, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        rms = np.expand_dims(rms, axis=0)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=padded, sr=44100, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "        mfccs = np.expand_dims(mfccs, axis=0)\n",
    "        \n",
    "        # Combine features\n",
    "        zcr_features = np.swapaxes(zcr, 1, 2)\n",
    "        rms_features = np.swapaxes(rms, 1, 2)\n",
    "        mfccs_features = np.swapaxes(mfccs, 1, 2)\n",
    "        \n",
    "        X_features = np.concatenate((zcr_features, rms_features, mfccs_features), axis=2)\n",
    "        \n",
    "        return X_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_single_audio_existing_model(model, audio_path):\n",
    "    \"\"\"\n",
    "    Test a single audio file with the existing model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model object\n",
    "    - audio_path: Path to the audio file\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_class: 'real' or 'fake'\n",
    "    - confidence: Confidence score\n",
    "    \"\"\"\n",
    "    X_features = preprocess_single_audio(audio_path)\n",
    "    \n",
    "    if X_features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X_features, verbose=0)[0]\n",
    "    \n",
    "    # Get class and confidence\n",
    "    predicted_class_idx = np.argmax(prediction)\n",
    "    confidence = prediction[predicted_class_idx]\n",
    "    \n",
    "    # Map class index to label\n",
    "    predicted_class = 'real' if predicted_class_idx == 1 else 'fake'\n",
    "    \n",
    "    print(f\"Audio: {os.path.basename(audio_path)}\")\n",
    "    print(f\"Prediction: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "def batch_test_existing_model(model, directory_path):\n",
    "    \"\"\"\n",
    "    Test a directory of audio files with the existing model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model object\n",
    "    - directory_path: Path to the directory containing audio files\n",
    "    \n",
    "    Returns:\n",
    "    - results: List of dictionaries with prediction results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Check if the directory has real/fake subdirectories for evaluation\n",
    "    has_ground_truth = False\n",
    "    if 'real' in os.listdir(directory_path) and 'fake' in os.listdir(directory_path):\n",
    "        has_ground_truth = True\n",
    "    \n",
    "    if has_ground_truth:\n",
    "        # Process files with known ground truth\n",
    "        for class_name in ['real', 'fake']:\n",
    "            class_dir = os.path.join(directory_path, class_name)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith(('.wav', '.mp3', '.m4a', '.flac')):\n",
    "                    audio_path = os.path.join(class_dir, file)\n",
    "                    predicted_class, confidence = test_single_audio_existing_model(model, audio_path)\n",
    "                    \n",
    "                    if predicted_class is not None:\n",
    "                        results.append({\n",
    "                            'file': file,\n",
    "                            'true_class': class_name,\n",
    "                            'prediction': predicted_class,\n",
    "                            'confidence': float(confidence),\n",
    "                            'correct': class_name == predicted_class\n",
    "                        })\n",
    "                        \n",
    "                        # For confusion matrix\n",
    "                        true_labels.append(1 if class_name == 'real' else 0)\n",
    "                        predicted_labels.append(1 if predicted_class == 'real' else 0)\n",
    "    else:\n",
    "        # Process files without known ground truth\n",
    "        for file in os.listdir(directory_path):\n",
    "            if file.endswith(('.wav', '.mp3', '.m4a', '.flac')):\n",
    "                audio_path = os.path.join(directory_path, file)\n",
    "                predicted_class, confidence = test_single_audio_existing_model(model, audio_path)\n",
    "                \n",
    "                if predicted_class is not None:\n",
    "                    results.append({\n",
    "                        'file': file,\n",
    "                        'prediction': predicted_class,\n",
    "                        'confidence': float(confidence)\n",
    "                    })\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total files analyzed: {len(results)}\")\n",
    "    \n",
    "    real_preds = [r for r in results if r['prediction'] == 'real']\n",
    "    fake_preds = [r for r in results if r['prediction'] == 'fake']\n",
    "    print(f\"Predicted as real: {len(real_preds)} ({len(real_preds)/len(results)*100:.1f}%)\")\n",
    "    print(f\"Predicted as fake: {len(fake_preds)} ({len(fake_preds)/len(results)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {np.mean([r['confidence'] for r in results]):.4f}\")\n",
    "    \n",
    "    # If we have ground truth, calculate accuracy metrics\n",
    "    if has_ground_truth:\n",
    "        correct_count = sum(1 for r in results if r.get('correct', False))\n",
    "        accuracy = correct_count / len(results)\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct_count}/{len(results)})\")\n",
    "        \n",
    "        # Display confusion matrix\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Display classification report\n",
    "        target_names = ['fake', 'real']\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, predicted_labels, target_names=target_names))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, target_names)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "        # Add text annotations to the confusion matrix\n",
    "        thresh = cm.max() / 2\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "test_existing_model(model, r\"dataset/for-norm/training/fake/file66.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav\", is_directory=False)\n",
    "\n",
    "# Directory test\n",
    "# test_existing_model(model, \"path/to/test_directory\", is_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ca8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Single File ===\n",
      "Model successfully loaded from deepfakedetectioncyber.keras\n",
      "Audio: file82.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav\n",
      "Prediction: fake\n",
      "Confidence: 0.9427\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def load_saved_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a saved Keras model from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: Path to the .keras model file\n",
    "    \n",
    "    Returns:\n",
    "    - The loaded model object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "        print(f\"Model successfully loaded from {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_single_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Preprocess a single audio file the same way as in training.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_path: Path to the audio file\n",
    "    \n",
    "    Returns:\n",
    "    - Feature vector ready for model input\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess audio\n",
    "        raw_audio = AudioSegment.from_file(audio_path)\n",
    "        samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n",
    "        trimmed, _ = librosa.effects.trim(samples, top_db=25)\n",
    "        padding = max(0, 50000 - len(trimmed))\n",
    "        padded = np.pad(trimmed, (0, padding), 'constant')\n",
    "        \n",
    "        # Extract features\n",
    "        FRAME_LENGTH = 2048\n",
    "        HOP_LENGTH = 512\n",
    "        \n",
    "        # Extract ZCR\n",
    "        zcr = librosa.feature.zero_crossing_rate(padded, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        zcr = np.expand_dims(zcr, axis=0)\n",
    "        \n",
    "        # Extract RMS\n",
    "        rms = librosa.feature.rms(y=padded, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n",
    "        rms = np.expand_dims(rms, axis=0)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=padded, sr=44100, n_mfcc=13, hop_length=HOP_LENGTH)\n",
    "        mfccs = np.expand_dims(mfccs, axis=0)\n",
    "        \n",
    "        # Combine features\n",
    "        zcr_features = np.swapaxes(zcr, 1, 2)\n",
    "        rms_features = np.swapaxes(rms, 1, 2)\n",
    "        mfccs_features = np.swapaxes(mfccs, 1, 2)\n",
    "        \n",
    "        X_features = np.concatenate((zcr_features, rms_features, mfccs_features), axis=2)\n",
    "        \n",
    "        return X_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_single_audio(model, audio_path):\n",
    "    \"\"\"\n",
    "    Test a single audio file with the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The loaded model object\n",
    "    - audio_path: Path to the audio file\n",
    "    \n",
    "    Returns:\n",
    "    - predicted_class: 'real' or 'fake'\n",
    "    - confidence: Confidence score\n",
    "    \"\"\"\n",
    "    X_features = preprocess_single_audio(audio_path)\n",
    "    \n",
    "    if X_features is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(X_features, verbose=0)[0]\n",
    "    \n",
    "    # Get class and confidence\n",
    "    predicted_class_idx = np.argmax(prediction)\n",
    "    confidence = prediction[predicted_class_idx]\n",
    "    \n",
    "    # Map class index to label\n",
    "    predicted_class = 'real' if predicted_class_idx == 1 else 'fake'\n",
    "    \n",
    "    print(f\"Audio: {os.path.basename(audio_path)}\")\n",
    "    print(f\"Prediction: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "def batch_test(model, directory_path):\n",
    "    \"\"\"\n",
    "    Test a directory of audio files with the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The loaded model object\n",
    "    - directory_path: Path to the directory containing audio files\n",
    "    \n",
    "    Returns:\n",
    "    - results: List of dictionaries with prediction results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Check if the directory has real/fake subdirectories for evaluation\n",
    "    has_ground_truth = False\n",
    "    if os.path.isdir(os.path.join(directory_path, 'real')) and os.path.isdir(os.path.join(directory_path, 'fake')):\n",
    "        has_ground_truth = True\n",
    "    \n",
    "    if has_ground_truth:\n",
    "        # Process files with known ground truth\n",
    "        for class_name in ['real', 'fake']:\n",
    "            class_dir = os.path.join(directory_path, class_name)\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.endswith(('.wav', '.mp3', '.m4a', '.flac')):\n",
    "                    audio_path = os.path.join(class_dir, file)\n",
    "                    predicted_class, confidence = test_single_audio(model, audio_path)\n",
    "                    \n",
    "                    if predicted_class is not None:\n",
    "                        results.append({\n",
    "                            'file': file,\n",
    "                            'true_class': class_name,\n",
    "                            'prediction': predicted_class,\n",
    "                            'confidence': float(confidence),\n",
    "                            'correct': class_name == predicted_class\n",
    "                        })\n",
    "                        \n",
    "                        # For confusion matrix\n",
    "                        true_labels.append(1 if class_name == 'real' else 0)\n",
    "                        predicted_labels.append(1 if predicted_class == 'real' else 0)\n",
    "    else:\n",
    "        # Process files without known ground truth\n",
    "        for file in os.listdir(directory_path):\n",
    "            if file.endswith(('.wav', '.mp3', '.m4a', '.flac')):\n",
    "                audio_path = os.path.join(directory_path, file)\n",
    "                predicted_class, confidence = test_single_audio(model, audio_path)\n",
    "                \n",
    "                if predicted_class is not None:\n",
    "                    results.append({\n",
    "                        'file': file,\n",
    "                        'prediction': predicted_class,\n",
    "                        'confidence': float(confidence)\n",
    "                    })\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Total files analyzed: {len(results)}\")\n",
    "    \n",
    "    real_preds = [r for r in results if r['prediction'] == 'real']\n",
    "    fake_preds = [r for r in results if r['prediction'] == 'fake']\n",
    "    print(f\"Predicted as real: {len(real_preds)} ({len(real_preds)/len(results)*100:.1f}%)\")\n",
    "    print(f\"Predicted as fake: {len(fake_preds)} ({len(fake_preds)/len(results)*100:.1f}%)\")\n",
    "    print(f\"Average confidence: {np.mean([r['confidence'] for r in results]):.4f}\")\n",
    "    \n",
    "    # If we have ground truth, calculate accuracy metrics\n",
    "    if has_ground_truth:\n",
    "        correct_count = sum(1 for r in results if r.get('correct', False))\n",
    "        accuracy = correct_count / len(results)\n",
    "        print(f\"Accuracy: {accuracy:.4f} ({correct_count}/{len(results)})\")\n",
    "        \n",
    "        # Display confusion matrix\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Display classification report\n",
    "        target_names = ['fake', 'real']\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, predicted_labels, target_names=target_names))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, target_names)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "        # Add text annotations to the confusion matrix\n",
    "        thresh = cm.max() / 2\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_saved_model(model_path, audio_path, is_directory=False):\n",
    " \n",
    "    model = load_saved_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to load model. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    # Test the model\n",
    "    if is_directory:\n",
    "        return batch_test(model, audio_path)\n",
    "    else:\n",
    "        return test_single_audio(model, audio_path)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    MODEL_PATH = \"deepfakedetectioncyber.keras\"\n",
    "   \n",
    "    print(\"\\n=== Testing Single File ===\")\n",
    "    result = test_saved_model(MODEL_PATH, r\"C:\\Users\\chawk\\Downloads\\file82.mp3.wav_16k.wav_norm.wav_mono.wav_silence.wav\", is_directory=False)\n",
    "    \n",
    "    # Test a directory\n",
    "    # print(\"\\n=== Testing Directory ===\")\n",
    "    # results = test_saved_model(MODEL_PATH, \"path/to/test_directory\", is_directory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
